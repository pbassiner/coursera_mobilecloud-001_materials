1
00:00:00,012 --> 00:00:06,708
Welcome to Part 8 in the module on
Patterns and Frameworks for Concurrency

2
00:00:06,708 --> 00:00:09,562
and Synchronization.
In the previous parts of this module, we

3
00:00:09,562 --> 00:00:11,146
first described the active object pattern
and illustrated how the ACE Task framework

4
00:00:11,146 --> 00:00:12,712
will be used to implement this pattern.
Within illustrated the application of ACE

5
00:00:12,712 --> 00:00:14,274
Task and ACE Acceptor-Connector and some
other Frameworks to implement a thread for

6
00:00:14,274 --> 00:00:15,882
connection version of our JAWS web server.
We then describe the Half-Sync, Half-Async

7
00:00:15,882 --> 00:00:17,356
pattern that combines Reactor and active
object and illustrated a number of ACE

8
00:00:17,356 --> 00:00:18,808
frameworks that could be combined to
implement this pattern and apply it in the

9
00:00:18,808 --> 00:00:19,845
context of jaws to provide a more scalable
solution.

10
00:00:19,845 --> 00:00:25,383
Within illustrated the application of ACE
Task and ACE Acceptor-Connector and some

11
00:00:25,383 --> 00:00:31,081
other frameworks to implement a thread for
connection version of our JAWS Web Server.

12
00:00:31,081 --> 00:00:36,769
We then describe the Half-Sync/Half-Async
pattern that combines Reactor and active

13
00:00:36,769 --> 00:00:41,746
object and illustrated a number of ACE
Frameworks that could be combined to

14
00:00:41,746 --> 00:00:47,276
implement this pattern and apply it in the
context of jaws to provide a more scalable

15
00:00:47,276 --> 00:00:50,563
solution.
We then describe the monographic pattern

16
00:00:50,563 --> 00:00:54,772
which was used to serialize access to a
synchronized request queue in our

17
00:00:54,772 --> 00:00:59,006
Half-Sync/Half-Async solution.
And also, illustrated ways in which

18
00:00:59,006 --> 00:01:03,830
various synchronization patterns can be
applied to monitor an object in the ACE

19
00:01:03,830 --> 00:01:08,870
message queue to make it more pluggable,
more scaleable, easier to use in different

20
00:01:08,870 --> 00:01:11,694
contexts.
What we're going to do now is turn our

21
00:01:11,694 --> 00:01:16,878
attention to one final concurrency pattern
in this particular module, which relates

22
00:01:16,878 --> 00:01:21,035
to something called Leaders/Follower,
Leader/Follower pattern.

23
00:01:21,035 --> 00:01:26,508
And this pattern is another way to provide
for pools of thread that access underlying

24
00:01:26,508 --> 00:01:29,267
event sources.
If you went back and looked at the

25
00:01:29,267 --> 00:01:33,555
Half-Sync/Half-Async solution, even though
we did a good job of addressing the

26
00:01:33,555 --> 00:01:38,077
limitations with the reactive approach,
where there was only one sort of control.

27
00:01:38,077 --> 00:01:42,107
Even though, we did a good job of being
able to be more effective in our use of

28
00:01:42,107 --> 00:01:46,332
resources than the thread per connection
approach where we used the reactive

29
00:01:46,332 --> 00:01:49,270
object, there still are potentially some
problems.

30
00:01:49,270 --> 00:01:53,293
Especially, if you have requests that
don't run for very long periods of time.

31
00:01:53,293 --> 00:01:57,045
And here's what some of the limitations
are with Half-Sync/Half-Async.

32
00:01:57,045 --> 00:02:01,335
One of the problems is that because the
thread that receives the incoming request

33
00:02:01,335 --> 00:02:05,556
is not the same thread that will process
it, we have to end up using dynamic memory

34
00:02:05,556 --> 00:02:10,155
allocation, which incurred synchronization
overhead and deals with, with memory

35
00:02:10,155 --> 00:02:13,498
fragmentation and free lists and all those
kinds of issues.

36
00:02:13,498 --> 00:02:17,802
Likewise, when we try to go ahead and NQ
and DQ things onto our thread safe

37
00:02:17,802 --> 00:02:22,509
synchronized request cue, we're going to
have to do synchronization operations.

38
00:02:22,509 --> 00:02:26,059
So, we'll have to grab a lock and that'll
have some overhead.

39
00:02:26,059 --> 00:02:30,688
Likewise, when we try to move things
between these threads, that will typically

40
00:02:30,688 --> 00:02:34,897
incur a context switch, which can take
many, many low-level instruction

41
00:02:34,897 --> 00:02:39,003
operations in order to be run.
And we'll deal with caches and other low

42
00:02:39,003 --> 00:02:43,132
level-hardware overheads.
Another issue, speaking of low-level

43
00:02:43,132 --> 00:02:47,857
hardware overheads, has to do with the
overhead of moving data such as these

44
00:02:47,857 --> 00:02:52,807
request messages between caches, between
cores, between CPUs, on a multi-CPU

45
00:02:52,807 --> 00:02:55,806
platform, and that all starts to add up as
well.

46
00:02:55,806 --> 00:03:01,137
So even though Half-Sync/Half-Async has
many positive qualities for certain types

47
00:03:01,137 --> 00:03:06,246
of request, it may be too much overhead.
So what can we do to address this problem?

48
00:03:06,246 --> 00:03:11,268
What we're going to describe here is how
to apply the Leader/Followers pattern in

49
00:03:11,268 --> 00:03:16,247
order to be able to address this issue.
Leader/followers is a pattern that allows

50
00:03:16,247 --> 00:03:20,867
a pool of threads, to efficiently and
predictably take turns accessing a lower

51
00:03:20,867 --> 00:03:25,052
level set of event endpoints.
Things that you might have with, say, the

52
00:03:25,052 --> 00:03:29,470
select system call that I've talked about
in the past or the poll system call.

53
00:03:29,470 --> 00:03:34,660
This particular pattern is structured in
the couple of ways we're going to talk

54
00:03:34,660 --> 00:03:36,907
about here.
Part of it, looks an awful lot like the

55
00:03:36,907 --> 00:03:39,470
Reactor.
You've got a set of handles that you're

56
00:03:39,470 --> 00:03:43,618
going to walk and manage, and, and
interact with, and monitor for different

57
00:03:43,618 --> 00:03:46,554
kinds of events, from different sources of
events.

58
00:03:46,554 --> 00:03:51,250
You've got a number of event handlers that
you can use to dispatch when things happen

59
00:03:51,250 --> 00:03:54,852
on those sets of handles.
There's also a set of concrete event

60
00:03:54,852 --> 00:03:59,612
handlers that you can inherit from the
abstract event handlers to do application

61
00:03:59,612 --> 00:04:04,097
or server specific processing logic.
And all of this sounds quite a bit like

62
00:04:04,097 --> 00:04:07,759
the reactor, of course.
But the one thing that's different from

63
00:04:07,759 --> 00:04:11,251
the classic Reactor pattern is the
presence of a thread pool.

64
00:04:11,251 --> 00:04:15,811
So we can actually have a pool of threads
that we'll call down into the hands, the

65
00:04:15,811 --> 00:04:20,089
handle set in order to take turns,
accessing the source of events that, that

66
00:04:20,089 --> 00:04:23,729
handle set encapsulates.
If you take a look at this URL, you'll

67
00:04:23,729 --> 00:04:27,106
find a paper that describes other aspects
of this pattern.

68
00:04:27,106 --> 00:04:31,199
Naturally, you can also read the
corresponding chapter in the posted two

69
00:04:31,199 --> 00:04:34,605
book for much, much more detail about how
all of this works.

70
00:04:34,605 --> 00:04:39,182
Let's now talk a bit about dynamics.
At first glance, the dynamics here look a

71
00:04:39,182 --> 00:04:42,324
little daunting.
But when we break it down, you'll see it's

72
00:04:42,324 --> 00:04:45,523
actually pretty straightforward.
So here's what happens.

73
00:04:45,523 --> 00:04:49,689
You end up spawning a pool of threads and
they all attempt to become the Leader

74
00:04:49,689 --> 00:04:52,452
thread.
Only one thread at a time can become the

75
00:04:52,452 --> 00:04:55,363
Leader.
And the leader will sit there and wait for

76
00:04:55,363 --> 00:04:59,913
something to happen on the source of
events perhaps using select and the other

77
00:04:59,913 --> 00:05:04,726
threads will then wait on some kind of
synchronizer or queue as Follower threads.

78
00:05:04,726 --> 00:05:09,596
When an event occurs, when an event
arrives, the Leader thread will take that

79
00:05:09,596 --> 00:05:13,036
request and go ahead and deal with it.
It will read it.

80
00:05:13,036 --> 00:05:17,842
It will process it and so on and so forth,
before it does that, it goes ahead and

81
00:05:17,842 --> 00:05:21,412
first promotes.
One of the Follower threads to become a

82
00:05:21,412 --> 00:05:24,333
Leader.
And when the Follower thread becomes a new

83
00:05:24,333 --> 00:05:27,710
leader, it will of course, wait for the
next set of events.

84
00:05:27,710 --> 00:05:32,408
While the original Leader thread morphs
itself into a so called processing thread

85
00:05:32,408 --> 00:05:35,799
to handle the request.
And notice that the thread now that

86
00:05:35,799 --> 00:05:39,643
handles the request is the same thread
that received the request.

87
00:05:39,643 --> 00:05:44,272
So we don't have those issues of Dynamic
memory allocation and not as much issues

88
00:05:44,272 --> 00:05:48,550
of synchronization and context switching
and data movement as we had with

89
00:05:48,550 --> 00:05:52,377
Half-Sync/Half-Async.
When the processing thread, that was a

90
00:05:52,377 --> 00:05:57,207
leader thread, finishes, it then typically
turns around and goes back to being new

91
00:05:57,207 --> 00:06:02,177
Follower thread to wait for its turn to
become the leader, in some implementations

92
00:06:02,177 --> 00:06:06,797
of this pattern you'll see that threads
that are done being processing threads

93
00:06:06,797 --> 00:06:11,318
actually become the new leader thread.
And that's a way of trying to enhance

94
00:06:11,318 --> 00:06:16,438
cache affinity and thread affinity so you
can keep things warmer and moving faster.

95
00:06:16,438 --> 00:06:20,447
This is also something that makes it a
little bit more responsive and quite

96
00:06:20,447 --> 00:06:24,534
different from the types of things that
humans would normally do where you would

97
00:06:24,534 --> 00:06:27,011
queue up in a queue and take turns in a
FIFA order.

98
00:06:27,011 --> 00:06:31,043
You can do a LIFO style model with the
leader, followers pattern and things work

99
00:06:31,043 --> 00:06:35,008
quite fine because threads don't care the
order in which they wait to do their work,

100
00:06:35,008 --> 00:06:39,992
they can all process things one at a time.
Let's now talk about how we could apply

101
00:06:39,992 --> 00:06:43,495
this particular pattern to JAWS.
What we can do is we can use a special

102
00:06:43,495 --> 00:06:46,029
framework that's part of ACE called the
ACE Thread Pool reactor or ACE TP Reactor.

103
00:06:46,029 --> 00:06:54,053
And this TP reactor is basically a reactor
with the special property that a pool of

104
00:06:54,053 --> 00:07:00,253
threads can call its run event loop, its
handle events method.

105
00:07:00,253 --> 00:07:05,158
And in that particular pool of threads,
then it uses a Leader/Followers pattern to

106
00:07:05,158 --> 00:07:09,702
have one thread at a time wait for work to
do while the other threads queue up as

107
00:07:09,702 --> 00:07:13,032
followers.
The nice thing about doing this particular

108
00:07:13,032 --> 00:07:17,928
approach is we can eliminate the need for
the extra thread in the async layer we had

109
00:07:17,928 --> 00:07:22,241
with Half-Sync/Half-Async.
We could also get rid of the synchronized

110
00:07:22,241 --> 00:07:25,371
message queue that we had at
Half-Sync/Half-Async.

111
00:07:25,371 --> 00:07:30,195
So, as a consequence, it will greatly
reduce the amount of overhead and make the

112
00:07:30,195 --> 00:07:34,057
system more predictable.
We're going to talk in a bit more, just a

113
00:07:34,057 --> 00:07:38,033
moment, about the benefits and limitations
of Leader/Followers.

114
00:07:38,033 --> 00:07:42,311
But I first want to talk about how, even
though, it will make the processing a

115
00:07:42,311 --> 00:07:47,279
little bit lower overhead, especially for
small, short duration requests, you might

116
00:07:47,279 --> 00:07:52,246
still consider using Half-Sync/Half-Async
for some environments, some contexts.

117
00:07:52,246 --> 00:07:57,148
In particular, Half-Sync/Half-Async does a
few things in a little different way than

118
00:07:57,148 --> 00:08:02,048
Leader/Followers, which may be beneficial.
One thing that Half-Sync/Half-Async does

119
00:08:02,048 --> 00:08:06,395
is it allows you to be able to reorder and
reprioritize incoming requests, because

120
00:08:06,395 --> 00:08:09,147
they sit on a separate synchronized
request queue.

121
00:08:09,147 --> 00:08:13,642
So you can deal with the requests by the
importance of the clients for example, as

122
00:08:13,642 --> 00:08:18,228
opposed to the order in which they arrive.
Another benefit you get with

123
00:08:18,228 --> 00:08:23,916
Half-Sync/Half-Async is the ability to be
able to dedicate user level virtual memory

124
00:08:23,916 --> 00:08:28,482
to queue up the requests.
You can allocate megabytes and gigabytes

125
00:08:28,482 --> 00:08:32,073
and so on of internal memory to queue
these things up.

126
00:08:32,074 --> 00:08:36,582
In contrast with Leader/Followers, the
queuing is typically done in the operating

127
00:08:36,582 --> 00:08:40,232
system protocol stack layer.
And there is not nearly as much memory

128
00:08:40,232 --> 00:08:44,947
available there on a per connection basis.
It's usually more like a 100K bytes or so

129
00:08:44,947 --> 00:08:47,631
before you end up filling up your Window
size.

130
00:08:47,631 --> 00:08:51,853
So an architect needs to know which of
these approaches to apply, and these

131
00:08:51,853 --> 00:08:56,477
patterns help you to navigate through the
alternative design space as part of our

132
00:08:56,477 --> 00:08:59,905
pattern language.
Let's now go and talk about the benefits

133
00:08:59,905 --> 00:09:04,119
and limitations of Leader/Followers.
Some of the benefits are, we end up

134
00:09:04,119 --> 00:09:07,951
getting tremendous performance
enhancements in certain areas.

135
00:09:07,951 --> 00:09:12,288
For example, having the one thread of
control wait for work to do, and then

136
00:09:12,288 --> 00:09:15,572
making sure that, that thread is where the
work is done.

137
00:09:15,572 --> 00:09:20,264
Can improve cache affinity for the thread,
which means that you don't need to move

138
00:09:20,264 --> 00:09:23,922
things between the caches.
It also means you don't have to even

139
00:09:23,922 --> 00:09:28,752
allocate memory dynamically oftentimes if
you have a big enough buffer in the stack

140
00:09:28,752 --> 00:09:33,303
frame in which the request comes in.
So that will improve and reduce certain

141
00:09:33,303 --> 00:09:36,870
sources of overhead that Half sync half
async has to incur.

142
00:09:36,870 --> 00:09:41,364
Likewise you can also minimize the amount
of locking overhead and the need to be

143
00:09:41,364 --> 00:09:45,784
able to exchange data between threads,
because we're not moving data between

144
00:09:45,784 --> 00:09:48,591
threads.
When the data comes it, it stays with the

145
00:09:48,591 --> 00:09:53,145
thread that originally detected it and is
going to read it and process it in some

146
00:09:53,145 --> 00:09:56,475
subsequent way when it morphs from being a
leader thread.

147
00:09:56,475 --> 00:09:59,840
So that can reduce the need for
synchronization Overhead.

148
00:09:59,840 --> 00:10:04,887
Likewise you can also end up reducing the
amount of priority inversion because we're

149
00:10:04,887 --> 00:10:09,502
not trying to cue things up in some way
that would end incurring additional

150
00:10:09,502 --> 00:10:12,736
overhead.
People often times will actually allocate

151
00:10:12,736 --> 00:10:17,179
multiple pools of threads each one running
their own thread pool reactor.

152
00:10:17,180 --> 00:10:21,112
At the appropriate priority level in order
to be able to maximize the priority

153
00:10:21,112 --> 00:10:24,818
preservation in a real time system.
And another reason why this can be a

154
00:10:24,818 --> 00:10:28,658
beneficial approach versus
Half-Sync/Half-Async is the amount of

155
00:10:28,658 --> 00:10:31,126
context switching is reduced for each
event.

156
00:10:31,126 --> 00:10:34,176
We are not moving things around between
the threads as much.

157
00:10:34,176 --> 00:10:38,018
So, that's a win as well.
Another nice thing is it makes it really

158
00:10:38,018 --> 00:10:41,697
to implement a thread pool using a
reactive style of design.

159
00:10:41,697 --> 00:10:46,661
In the next part, in this module, we will
give you an illustration of how to do that

160
00:10:46,661 --> 00:10:50,193
with our JAWS Web Server.
Naturally, of course, there are

161
00:10:50,193 --> 00:10:53,004
limitations.
This particular pattern is very

162
00:10:53,004 --> 00:10:57,218
complicated to Implement.
We've implemented many variations of it in

163
00:10:57,218 --> 00:11:00,503
ACE over the years and I think we've got
all the bugs out.

164
00:11:00,503 --> 00:11:04,175
But it took us a while because there's
subtleties, there's different variance of

165
00:11:04,175 --> 00:11:07,847
Leader/Followers, there's a bound variant,
there's an unbound variant that are

166
00:11:07,847 --> 00:11:11,573
described in the various papers and it's
important to understand these different

167
00:11:11,573 --> 00:11:15,421
variations to implement it effectively.
And naturally, it also helps if you have a

168
00:11:15,421 --> 00:11:18,853
framework that does the work for you so
you don't have to do it yourself.

169
00:11:18,854 --> 00:11:22,609
Another downside is it's a bit less
flexible than Half-Sync/Half-Async.

170
00:11:22,609 --> 00:11:26,946
You can't easily discard the messages, you
can't easily reprioritize them, because

171
00:11:26,946 --> 00:11:30,491
there's no extra queue, there's no extra
thread in which to do that.

172
00:11:30,491 --> 00:11:34,706
So that can be a little tricky, and of
course, because we have one thread of

173
00:11:34,706 --> 00:11:39,191
control that's waiting at a time for
incoming requests on a set of handles that

174
00:11:39,191 --> 00:11:42,956
itself could become a bottleneck in highly
scalable systems.

175
00:11:42,956 --> 00:11:48,389
So as a general rule of thumb, I like to
use Leader/Followers in situations where

176
00:11:48,389 --> 00:11:53,978
I've got more real-time responsiveness,
where predictability is more important

177
00:11:53,978 --> 00:11:59,648
than scalability and I like to be able to
use Half-Sync/Half-Async situations with

178
00:11:59,648 --> 00:12:05,399
the reverse properties where scalability
is very important and predictibility is a

179
00:12:05,399 --> 00:12:10,303
little bit less important.
Next, we'll talk about how to illustrate

180
00:12:10,303 --> 00:12:12,946
this pattern being applied to JAWS.
